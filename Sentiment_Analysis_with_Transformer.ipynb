{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A good reference to compare results: https://towardsdatascience.com/sentiment-analysis-a-benchmark-903279cab44a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-addons in /opt/conda/lib/python3.8/site-packages (0.13.0)\r\n",
      "Requirement already satisfied: typeguard>=2.7 in /opt/conda/lib/python3.8/site-packages (from tensorflow-addons) (2.12.1)\r\n"
     ]
    }
   ],
   "source": [
    "#install addons for \n",
    "!pip install tensorflow-addons\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import csv \n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Embedding, Dense, Input, Dropout, LayerNormalization, GlobalAveragePooling1D, Flatten\n",
    "from tensorflow_addons.layers import MultiHeadAttention\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data from Kaggle https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/tasks?taskId=588 \n",
    "# Load the 1mdb datasets from Kaggle\n",
    "kaggle_imdb_file = 'datasets/IMDB_Dataset.csv' \n",
    "data_x = []\n",
    "data_y = []\n",
    "with open(kaggle_imdb_file, 'r') as csvfile: \n",
    "  filereader = csv.reader(csvfile, delimiter=',', dialect='excel')\n",
    "  next(filereader)\n",
    "  for row in filereader:\n",
    "    data_x.append(row[0]) \n",
    "    label = 1 if row[1] == 'positive' else 0 \n",
    "    data_y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data into trainable format\n",
    "data_x = np.array(data_x)\n",
    "data_y = np.array(data_y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.2, random_state=29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : I guess this is the first time I have seen a Roscoe 'Fatty' Arbuckle movie. I really liked him in his (title) role as a butcher boy. The way he moves is very funny in my opinion, for example how he handles his knife and the way he rolls a cigarette. I think he is a good actor; his facial expressions really suit the role he plays, for example how he winks at the audience in the end. But one might add that that was probably not too difficult. Anyway I think he would have deserved a longer career. As you probably know it was ruined by greedy journalists who made money by printing false accusations that said he was involved in a scandal.<br /><br />The plot is not very important. In the first half, Fatty and Alum are employees at a store and rivals for Almondine's affection. After a heavy food fight, Almondine is sent to a girls' school by her father, the store owner. (This is the beginning of the second half). Both Fatty and Alum enter the school in drag, and the fight for Almondine continues. (Some of the characters' names are different in the version that I have seen. It seems that for some reason they replaced the original title cards with new ones.)<br /><br />There are a lot of corny gags like food fights and pratfalls, but they are done well in my opinion. And there are some gags I really liked, for example how they make the dog run the pepper mill (or is it a coffee mill?), or the scene when Fatty dons a coat although it is obviously not necessary, or when Miss Teachem, the head of the girls' school, spanks Fatty, and he spanks her back.<br /><br />Buster Keaton is also funny in this, his first, movie; a good addition to the cast. In the first half he is a customer at the store, in the second half he supports Alum in his fight for Almondine. I liked his acrobatics, for example when Fatty pushes him from one room of the school to another, he doesn't show a simple pratfall but lands on his hands and his head and does a little pirouette. Watch out for one scene in the food fight: Alum throws a flour bag at him, but it misses and hits the store owner instead. That makes Buster laugh, which must be a rarity since he normally always shows a neutral expression (which - as you probably also know already - got him the nickname 'The Great Stone Face'). (One more note: Al St. John, who plays Alum, was 'Fatty' Arbuckle's nephew, and later became famous for the role of 'Fuzzy' that he played in lots of westerns.)<br /><br />I don't like this one as much as I like, for example, 'One Week' and 'The Balloonatic' (films that Buster made later, without 'Fatty'). And it didn't make me laugh out loud often - but it made me smile a lot, so I have given it eight points.\n",
      "\n",
      "=====================\n",
      "\n",
      "1 : \"I have looked into the eye of this island, and what I saw was beautiful,\" proclaims one of the main characters in ABC's award winning television show \"Lost\". The series could be summarized as a drama story about a group of plane crash survivors stranded on an unknown island, but that would be doing the show a disservice. \"Lost\" follows a large group of characters who come into conflict with the island, each other, and ultimately themselves as they struggle with their new way of life and their dependency on each other. The situation becomes more complicated when it becomes clear this isn't an ordinary island, either - and that they may not be alone.<br /><br />My initial fear after hearing the concept of this series was the lack of new stories they could tell us after a certain period, but this proved to be unfounded. The narrative flows naturally, the dialogue is witty, the characters are memorable and the execution is superb. The island is a character all on its own, and to understand this comment you'd have to see the series for yourself, which only goes to show its originality and greatness.<br /><br />At the time of writing this review, only the first two seasons have aired, and they're filled with strong episodes. My only mild criticism is that the second season seems to slow down a bit halfway, but then fortunately comes back in admirable shape for the final episodes.<br /><br />If I can recommend one television series you should be following right now, it would certainly be this one. If you like excitement, adventure, character driven stories, an extremely strong cast and crew, beautiful locations, and an island that seems more spiritual than natural, \"Lost\" is for you. Just be sure you start at the beginning.\n",
      "\n",
      "=====================\n",
      "\n",
      "1 : This film has very tight and well planned dialogue, acting and choreography.<br /><br />Recommended film for anyone who wants to see masterful writing and plot.<br /><br />Question: Does anyone know where the house is actually located? It is one of the most interesting houses, a 19thC windmill.\n",
      "\n",
      "=====================\n",
      "\n",
      "1 : Steven Spielberg produced, wrote, came up with ideas for and even directed episodes of Amazing Stories, so naturally this would have to be the greatest anthology ever right? Unfortunately wrong. Some episodes are just fantastic, but all too often it was a mixed bag. In fact, that might have been it's downfall is it was way too mixed. Some episodes were light comedies, some were dramas, some were horror, and one was even animated, which made this a similar, but not as good 80s version of the Twilight Zone (which also was around).<br /><br />Normally I'd like having a mixture of stories in an anthology show, but they just didn't fully work here. Some of the more fantastical dramatic episodes felt like they would be better being shown late on night on the Lifetime network, like the episode \"Ghost Train\", which was directed by Spielberg himself. In that episode, it gave the message of hope, and gave us a fantasy story, but overall it was just a build up to the ending which didn't blow me away anyways. The horror episodes tended to work better than the drama, but there were far more dramatic ones, and they grow tiring to watch. Acting wise, this anthology got some big stars, similar to the original Twilight Zone. Kevin Costner, Kiefer Sutherland, Milton Berle, Dom Deluise, Harvey Keitel, Beau Bridges, Charlie Sheen, Forrest Whitaker, Tim Robbins, John Lithgow, Rhea Perlman, Danny Devito, Patrick Swayze, Christopher Lloyd, June Lockhart, Kathy Baker, Weird Al Yankovich and many other well knowns have been in episodes of the show. It's fun to see well known actors in almost every episode of the series. Great directors have also had part in episodes including Spielberg himself, Clint Eastwood, Burt Reynolds, Bob Clark, Joe Dante, Mick Garris, Paul Bartel, Joe Dante, Robert Zemeckis, Danny Devito and even Martin Scorsese. I'd actually recommend this more to fans of the directors and/or the 80s than anyone else.<br /><br />Amazing Stories was sometimes amazing, usually good, occasionally mediocre, and every once in a while a real stinker came out. But, this show has nostalgic value to me, and it's sort of fun to sit on boring afternoons and watch some episodes. John Williams' theme music for the show is sure to be caught in anyone's head who watches this, too.<br /><br />My rating: Good show. 30 mins. per episode. TVPG\n",
      "\n",
      "=====================\n",
      "\n",
      "1 : I thought that ROTJ was clearly the best out of the three Star Wars movies. I find it surprising that ROTJ is considered the weakest installment in the Trilogy by many who have voted. To me it seemed like ROTJ was the best because it had the most profound plot, the most suspense, surprises, most emotional,(especially the ending) and definitely the most episodic movie. I personally like the Empire Strikes Back a lot also but I think it is slightly less good than than ROTJ since it was slower-moving, was not as episodic, and I just did not feel as much suspense or emotion as I did with the third movie.<br /><br />It also seems like to me that after reading these surprising reviews that the reasons people cited for ROTJ being an inferior film to the other two are just plain ludicrous and are insignificant reasons compared to the sheer excellence of the film as a whole. I have heard many strange reasons such as: a) Because Yoda died b) Because Bobba Fett died c) Because small Ewoks defeated a band of stormtroopers d) Because Darth Vader was revealed<br /><br />I would like to debunk each of these reasons because I believe that they miss the point completely. First off, WHO CARES if Bobba Fett died??? If George Lucas wanted him to die then he wanted him to die. Don't get me wrong I am fan of Bobba Fett but he made a few cameo appearances and it was not Lucas' intention to make him a central character in the films that Star Wars fans made him out to be. His name was not even mentioned anywhere in the movie... You had to go to the credits to find out Bobba Fett's name!!! Judging ROTJ because a minor character died is a bit much I think... Secondly, many fans did not like Yoda dying. Sure, it was a momentous period in the movie. I was not happy to see him die either but it makes the movie more realistic. All the good guys can't stay alive in a realistic movie, you know. Otherwise if ALL the good guys lived and ALL the bad guys died this movie would have been tantamount to a cheesy Saturday morning cartoon. Another aspect to this point about people not liking Yoda's death.. Well, nobody complained when Darth Vader struck down Obi Wan Kenobi in A New Hope. (Many consider A New Hope to be the best of the Trilogy) Why was Obi Wan's death okay but Yoda's not... hmmmmmmmmmmmm.... Another reason I just can not believe was even stated was because people found cute Ewoks overpowering stormtroopers to be impossible. That is utterly ridiculous!! I can not believe this one!! First off, the Ewoks are in their native planet Endor so they are cognizant of their home terrain since they live there. If you watch the movie carefully many of the tactics the Ewoks used in defeating the stormtroopers was through excellent use of their home field advantage. (Since you lived in the forest all your life I hope you would have learned to use it to your advantage) They had swinging vines, ropes, logs set up to trip those walkers, and other traps. The stormtroopers were highly disadvantaged because they were outnumbered and not aware of the advantages of the forest. The only thing they had was their blasters. To add, it was not like the Ewoks were battling the stormtroopers themselves, they were heavily assisted by the band of rebels in that conquest. I thought that if the stormtroopers were to have defeated a combination of the Star Wars heros, the band of rebels, as well as the huge clan of Ewoks with great familiarity of their home terrain, that would have been a great upset. Lastly, if this scene was still unbelievable to you.. How about in Empire Strikes Back or in A New Hope where there were SEVERAL scenes of a group consisting of just Han Solo, Chewbacca, and the Princess, being shot at by like ten stormtroopers and all their blasters missed while the heros were in full view!! And not only that, the heroes , of course, always hit the Stormtroopers with their blasters. The troopers must have VERY, VERY bad aim then! At least in Empire Strikes Back, the Battle of Endor was much more believable since you had two armies pitted each other not 3 heroes against a legion of stormtroopers. Don't believe me? Check out the battle at Cloud City when our heroes were escaping Lando's base. Or when our heros were rescuing Princess Leia and being shot at (somehow they missed)as Han Solo and Luke were trying to exit the Death Star.<br /><br />The last reason that I care to discuss (others are just too plain ridiculous for me to spend my time here.) is that people did not like Darth Vader being revealed! Well, in many ways that was a major part of the plot in the movie. Luke was trying to find whether or not Darth Vader was his father, Annakin Skywalker. It would have been disappointing if the movie had ended without Luke getting to see his father's face because it made it complete. By Annakin's revelation it symbolized the transition Darth Vader underwent from being possessed by the dark side (in his helmet) and to the good person he was Annakin Skywalker (by removing the helmet). The point is that Annakin died converted to the light side again and that is what the meaning of the helmet removal scene was about. In fact, that's is what I would have done in that scene too if I were Luke's father...Isn't that what you would have done if you wanted to see your son with your own eyes before you died and not in a mechanized helmet?<br /><br />On another note, I think a subconscious or conscious expectation among most people is that the sequel MUST be worse (even if it is better) that preceding movies is another reason that ROTJ does not get as many accolades as it deserves. I never go into a film with that deception in mind, I always try to go into a film with the attitude that \"Well, it might be better or worse that the original .. But I can not know for sure.. Let's see.\" That way I go with an open mind and do not dupe myself into thinking that a clearly superior film is not as good as it really was.<br /><br />I am not sure who criticizes these movies but, I have asked many college students and adults about which is their favorite Star Wars movie and they all tell me (except for one person that said that A New Hope was their favorite) that it is ROTJ. I believe that the results on these polls are appalling and quite misleading.<br /><br />Bottom line, the Return of the Jedi was the best of the Trilogy. This movie was the only one of the three that kept me riveted all throughout its 135 minutes. There was not a moment of boredom because each scene was either suspenseful, exciting, surprising, or all of the above. For example, the emotional light saber battle between Luke and his father in ROTJ was better than the one in the Empire Strikes Back any day!!!<br /><br />Finally, I hope people go see the Phantom Menace with an open mind because if fans start looking for nitpicky, insignificant details (or see it as \"just another sequel\") to trash the movie such as \"This movie stinks because Luke is not in it!\" then this meritorious film will become another spectacular movie that will be the subject of derision like ROTJ suffered unfortunately.<br /><br />\n",
      "\n",
      "=====================\n",
      "\n",
      "1 : \"L'Auberge Espagnole\" collected the audience wherever it was shown. It gathered audience awards on many film festivals all over the world. And it is not strange. We have the ability to watch a cheerful and an astonishing piece of art. And it is wise by the way. \"L'Auberge Espagnole\" is a very funny comedy about youth and growing up. But most of all it is about the lights and shadows of living in the European Union.<br /><br />The main character of the film is a French student of economy Xavier. For his future carrier his is sent for one year of studying to Barcelona. In Spain it turns out that the lectures are being given in Catalonian language. That probably doesn't help the increasement of knowledge. But it helps in tightening the relationships inside the group of foreign exchange students. Especially if they rent a big flat together. There are 3 girls: English, Belgian and Spanish, as well as three boys: German, Danish and Italian. Our French guy will also get there. A year is a very long time. Long enough to get close and make friends. And get to know some European stereotypes while trying to break them apart.<br /><br />Klapisch treats this special case of a process of uniting Europe with humor and without pecky didactism. He comes out of the idea that young people are everywhere just the same. They like jokes. They like to make irresponsible relationships. But they don't neglect their aspirations. The most interesting is the sum of experience of this little community. They live together in the fire of everyday tasks fighting with the surrounding reality. They are full of unusual ideas for life. Young Europeans come back to their countries to take up a life of an adult on their own. They are Europe's hope to fight the many problems of the Union. For example, the terrifying administration system. In the end they proof that not only can they communicate and make friends despite the many differences. But they also now how to live the full of life. And they won't allow taking that full of life away from them.\n",
      "\n",
      "=====================\n",
      "\n",
      "0 : It's difficult to criticize a movie with the title like 'Deathbed: The Bed that Eats' and involves a ghost narrator who's trapped behind a 2-way painting he drew and a bed that snores and  if I'm not mistaken, masturbates. (Now, that's getting back at its human companions!) Furthermore, it foams up (in orange, for whatever reason) to absorb edibles lying on its surface, including apples, wine, fried chicken and, of course, people. Again it's suffice to say, that don't expect too much when you see what I guess is stomach acid  the final remains of anything that orange suds takes  dissolving only certain things. It'll drink the wine, but the bottle's okay and it'll eat away at the chicken bone, but the bucket's just fine. Heck, the bed even replaces the unused containers. Hilariously, at one point it downs Pepto-Bismol. I had to laugh at that one. I don't think they really wanted you to take any of this seriously. It's low budget, and it's extremely easy to see where they cut costs and saved oodles amounts of money. I thought, in a world where there can be a killer 'Lift' and a 'Blood Beach,' this 'Deathbed' might be amusing to watch. For reasons that might involve cost, 90% of the film is voice-over, no one screams or shows extremely low signs of fright/confusion on why a bed would attack (I can think of one  and I never was one of those kids that jumped on the bed) and you'll have to suspend your disbelief beyond belief. (A victim loses all flesh on his hands, barely saying \"ow.\") Only one scene, that went on too long, was minutely tense  a woman attempts to crawl away only to be dragged back, using a sheet. Where are the MST3k guys?\n",
      "\n",
      "=====================\n",
      "\n",
      "0 : This movie appears to have made for the sole purpose of annoying me. Everything I hate about films is present: fake sentimentality, extreme corniness, bad child actors and more feature abundantly. That's ignoring the fact that it depicts the extreme ignorance of American sports fans, with many of the cast professing that a football is shaped like a lemon. What?! That's a Rugby ball. The story follows a group of no hopers that get a new teacher that they like (who, coincidently, teaches the class in a short skirt) and gets them interested in football. Naturally, they're all rubbish (don't forget, they're no hopers) except for one kid who has moved from El Paso. Blah Blah, etc etc and the kids still don't become good footballers, but good heart ensues and the no hopers are turned into a bunch of well-rounded kids. Hell, even the adults start to come round; drunks are turned into caring parents, illegal immigrants are let off the hook...groan.<br /><br />This movie stars Steve Guttenberg. Now, before you go rushing off down your local video store to grab yourself a copy, hold up a minute. Guttenberg is rubbish. No, no; come on let's face it, how did this guy ever get to be in a movie? I have absolutely no idea, and there is nothing in this movie to give me an idea. Olivia d'Abo stars along side Steve and doesn't impress either. She merely seems to be going through the motions and looking nice while doing it. Although I have no problems with the latter part; her performance does the movie no credit. The child actors that make up the rest of the cast are just as bad as you would expect from a movie like this. Most of them are disgusting and/or annoying and it doesn't make for pleasant viewing at all. There's a goat in the film who plays the mascot and he does a good job; but you wouldn't see a movie for a goat, so don't bother seeing this movie.\n",
      "\n",
      "=====================\n",
      "\n",
      "1 : I didn't at all think of it this way, but my friend said the first thing he thought when he heard the title \"Midnight Cowboy\" was a gay porno. At that point, all I had known of it was the reference made to it in that \"Seinfeld\" episode with Jerry trying to get Kramer to Florida on that bus and Kramer's all sick and with a nosebleed.<br /><br />The movie was great, and surprisingly upbeat and not all pissy pretentious pessimistic like some movies I can't even remember because they're all crap.<br /><br />The plot basically consisted of a naive young cowboy Joe Buck going to New York trying to be a hustler (a male prostitute, basically), thinking it'll be easy pickings, only to hit the brick wall hard when a woman ends up hustling HIM, charging him for their sexual encounter.<br /><br />Then he meets Enrico Salvatore Rizzo, called \"Ratso\" by everyone and the cute gay guys who make fun of him all the time. You think of him as a scoundrel, but a lovable one (like Han Solo or Lando Calrissian) and surprisingly he and Joe become friends, and the movie is so sweet and heartwarming watching them being friendlier and such and such. Rizzo reveals himself to actually be a sad, pitiable man who's very sick, and very depressed and self-conscious, hates being called \"Ratso\" and wants to go to Florida, where he thinks life will be much better and all his problems resolved, and he'll learn to be a cook and be famous there.<br /><br />It's heartwarming watching Joe do all that he does to get them both down to Florida, along with many hilarious moments (like Ratso trying to steal food at that hippie party, and getting caught by the woman who says \"Gee, well, you know, it's free. You don't have to steal it.\" and he says \"Well if it's free then I ain't stealin' it\", and that classic moment completely unscripted and unscheduled where Hoffman almost gets hit by that Taxi, and screams \"Hey, I'm walkin' here! I'm walkin' here!\"), and the acting is so believable, you'd never believe Joe Buck would grow up to be the distinguished and respected actor Jon Voight, and Ratso Rizzo would grow up to be the legendary and beloved Dustin Hoffman. It's not the first time they've worked together in lead roles, but the chemistry is so thick and intense.<br /><br />Then there's the sad part that I believe is quite an overstatement to call it \"depressing\". Ratso Rizzo is falling apart all throughout the movie, can barely walk, barely eat, coughs a lot, is sick, and reaches a head-point on the bus on its way to Florida. He's hurting badly, and only miles away from Miami, he finally dies on the bus. The bus driver reassures everyone that nothing's wrong, and continues on. Sad, but not in the kind of way that'd make you go home and cry and mope around miserably as though you've just lost your dog of 13 years.<br /><br />All in all, great movie. And the soundtrack pretty much consists just of \"Everybody's Talking'\" played all throughout the movie at appropriate times. An odd move, but a great one, as the song is good and fits in with the tone of the movie perfectly. Go see it, it's great, go buy it\n",
      "\n",
      "=====================\n",
      "\n",
      "1 : The Patriot is a well thought out, well produced film, that will draw the viewer into the story directly, and keep them involved to the very end. Granted, it is not in the same vein as Under Siege, et. al., but this is definitely the trademark film of what might be described as Seagal's second era of films. Not so much Action movies with a nod to a story thrown in for good measure, but a good story, matched with a point, and injected with some of Seagal's unimitable action sequences. Is this the kind of film that hardcore action junkies will enjoy? Probably not. But for those of us who not only like our action, but also can appreciate a well told story, this is a big step forward for Seagal. It's almost like the transformation Clint Eastwood made as he tried to step out from underneath the shadow of his \"Dirty Harry\" series. Either man, had he continued in the singular direction they were headed, would have forever been keyholed in in specific role and guaranteed the brevity of their careers...\n",
      "\n",
      "=====================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show some sample reviews \n",
    "ids = np.random.randint(X_train.shape[0], size=10)\n",
    "for i in ids:\n",
    "  print(y_train[i], ':',  X_train[i])\n",
    "  print(\"\\n=====================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 124252 unique tokens.\n",
      "(40000, 400)\n",
      "(10000, 400)\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "MAX_SEQUENCE_LENGTH = 400\n",
    "MAX_NB_WORDS = 20000\n",
    "# Tokenize the data \n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(data_x)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_seq, padding='post', truncating='post',  maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test_padded = pad_sequences(X_test_seq, padding='post', truncating='post', maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(X_train_padded.shape)\n",
    "print(X_test_padded.shape)\n",
    "inverted_word_index = dict((i, word) for (word, i) in word_index.items())\n",
    "inverted_word_index[0] = ' '\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : I watched \"Fuckland\" a long time ago. I lied if I'd tell that I remember it in detail; what I remember most vividly is the irritation it provoked me and the feeling of a total waste of precious money and time, not only my time and money invested in watching the movie but also the director's.<br /><br />Supposedly, \"Fuckland\" is a critic of Argentinians, presenting us (I'm an Argentinian too) as little people who take credit for and even boast about petty, ridiculous victories, and think we're the best thing that God (who is also an Argentinian) created. I'm not going to argue that. It's probably a true statement about a quite big part of the population (the part I despise, by the way). And even if this weren't true, that's not my point. The worst sin \"Fuckland\" committed was to express such a statement about its own director.<br /><br />The continuous impression I received was that the director was too busy trying to impress us for sneaking a camera inside the islands to worry about making a good (even a mediocre) movie. Many of the takes made with a hidden camera are pointless. The director chooses to show off with a silly edition of old war takes and his own ones. And there's no plot at all.<br /><br />Moreover, this movie proudly presents a Dogme certificate before the opening titles, only to disrespect its principles afterwards (for example, by including the director in the credits - another sign of his pride?).<br /><br />I found the movie offensive, not as an Argentinian, but as a watcher. I felt underestimated. \"Fuckland\" is simply one of the worst movies I've ever seen.\n",
      "\n",
      "===============\n",
      "\n",
      "277 i watched a long time ago i lied if i'd tell that i remember it in detail what i remember most vividly is the irritation it provoked me and the feeling of a total waste of precious money and time not only my time and money invested in watching the movie but also the director's br br supposedly is a critic of presenting us i'm an argentinian too as little people who take credit for and even boast about petty ridiculous and think we're the best thing that god who is also an argentinian created i'm not going to argue that it's probably a true statement about a quite big part of the population the part i despise by the way and even if this weren't true that's not my point the worst sin committed was to express such a statement about its own director br br the continuous impression i received was that the director was too busy trying to impress us for sneaking a camera inside the islands to worry about making a good even a mediocre movie many of the takes made with a hidden camera are pointless the director chooses to show off with a silly edition of old war takes and his own ones and there's no plot at all br br moreover this movie proudly presents a dogme certificate before the opening titles only to disrespect its principles afterwards for example by including the director in the credits another sign of his pride br br i found the movie offensive not as an argentinian but as a watcher i felt underestimated is simply one of the worst movies i've ever seen                                                                                                                                                                                                                                                      \n"
     ]
    }
   ],
   "source": [
    "# check the decoded sequences\n",
    "idx = 21\n",
    "decoded_sequence = \" \".join(inverted_word_index[i] for i in X_train_padded[idx])\n",
    "print(y_train[idx], \":\", X_train[idx])\n",
    "print(\"\\n===============\\n\")\n",
    "print(len(X_train_seq[idx]), decoded_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "d_model: %s (100,)\n"
     ]
    }
   ],
   "source": [
    "# Loading Word Embedding Index \n",
    "# Download Glove Data from https://nlp.stanford.edu/data/glove.6B.zip and place the data under datasets/\n",
    "embeddings_index = {}\n",
    "GLOVE_DIR = \"datasets\"\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "print('d_model: %s', embeddings_index['hi'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124253, 100)\n"
     ]
    }
   ],
   "source": [
    "# Make the word embedding layer that can take X_train_padded as input \n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 400)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 400, 100)          12425300  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 40000)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 40000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 40)                1600040   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                820       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 14,026,181\n",
      "Trainable params: 14,026,181\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Train a FNN to fine tune the embeddings \n",
    "# FNN with embedding \n",
    "inputs = layers.Input(shape=(MAX_SEQUENCE_LENGTH,)) \n",
    "ffn_embedding_layer =  Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "                            trainable=True)\n",
    "outputs_fnn = ffn_embedding_layer(inputs)\n",
    "outputs_fnn = Flatten()(outputs_fnn) \n",
    "outputs_fnn = Dropout(0.1)(outputs_fnn)\n",
    "outputs_fnn = Dense(40, activation='relu')(outputs_fnn)\n",
    "outputs_fnn = Dropout(0.1)(outputs_fnn)\n",
    "outputs_fnn = Dense(20, activation='relu')(outputs_fnn)\n",
    "outputs_fnn = Dense(1, activation='sigmoid')(outputs_fnn) \n",
    "\n",
    "model_fnn = keras.Model(inputs=inputs, outputs=outputs_fnn) \n",
    "model_fnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model_fnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1000/1000 [==============================] - 151s 151ms/step - loss: 0.4876 - accuracy: 0.7558 - val_loss: 0.3581 - val_accuracy: 0.8447\n",
      "Epoch 2/2\n",
      "1000/1000 [==============================] - 155s 155ms/step - loss: 0.2421 - accuracy: 0.9034 - val_loss: 0.3209 - val_accuracy: 0.8641\n"
     ]
    }
   ],
   "source": [
    "history_fnn = model_fnn.fit(\n",
    "    X_train_padded, y_train, batch_size=40, epochs=2, validation_data=(X_test_padded, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract out the embedding from the fully conncted model as input to the Transformer\n",
    "ffn_embeddings = ffn_embedding_layer.get_weights()[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare to Train a Transformer \n",
    "# Helper functions with padding_masks and positional encodings\n",
    "# Now let's handle positional encoding\n",
    "def positional_encoding(positions, d):\n",
    "    \"\"\"\n",
    "    Precomputes a matrix with all the positional encodings \n",
    "    \n",
    "    Arguments:\n",
    "        positions (int) -- Maximum number of positions to be encoded \n",
    "        d (int) -- Encoding size \n",
    "    \n",
    "    Returns:\n",
    "        pos_encoding -- (1, position, d_model) A matrix with the positional encodings\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize a matrix angle_rads of all the angles \n",
    "    angle_rads = np.arange(positions)[:, np.newaxis] / np.power(10000, (2 * (np.arange(d)[np.newaxis, :]//2)) / np.float32(d))\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# Now we need to create masks for padded words\n",
    "def create_padding_mask_deprecated(decoder_token_ids):\n",
    "    \"\"\"Deprecated This No Longer Works\"\"\"\n",
    "    \"\"\"\n",
    "    Creates a matrix mask for the padding cells\n",
    "    \n",
    "    Arguments:\n",
    "        decoder_token_ids -- (n, m) matrix\n",
    "    \n",
    "    Returns:\n",
    "        mask -- (n, 1, 1, m) binary tensor\n",
    "    \"\"\"    \n",
    "    seq = 1 - tf.cast(tf.math.equal(decoder_token_ids, 0), tf.float32)\n",
    "  \n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, :] \n",
    "\n",
    "def create_padding_mask(decoder_token_ids):\n",
    "    \"\"\"This works with tensorflow_addons\"\"\"\n",
    "    \"\"\"\n",
    "    Creates a matrix mask for the padding cells\n",
    "    \n",
    "    Arguments:\n",
    "        decoder_token_ids -- (n, m) matrix\n",
    "    \n",
    "    Returns:\n",
    "        mask -- (n, m, m) binary tensor\n",
    "    Discussions at https://github.com/tensorflow/tensorflow/issues/49237 \n",
    "    \"\"\"    \n",
    "    def outer_product(a):\n",
    "      return tf.tensordot(a, a, 0)\n",
    "    \n",
    "    seq = 1 - tf.cast(tf.math.equal(decoder_token_ids, 0), tf.float32)\n",
    "    \n",
    "    res = tf.cast(tf.vectorized_map(outer_product, seq), dtype=bool) \n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return res\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to build a decoder only Transformer for sentiment classification\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The decoder layer is composed by a multi-head self-attention mechanism,\n",
    "    followed by a simple, positionwise fully connected feed-forward network. \n",
    "    This archirecture includes a residual connection around each of the two \n",
    "    sub-layers, followed by layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim,\n",
    "                 dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(num_heads=num_heads,\n",
    "                                      head_size=embedding_dim, #key_dim=embedding_dim,\n",
    "                                      dropout=dropout_rate)\n",
    "\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(fully_connected_dim, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "          tf.keras.layers.Dense(embedding_dim)  # (batch_size, seq_len, d_model)\n",
    "        ])\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=layernorm_eps)\n",
    "\n",
    "        self.dropout_ffn = Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder Layer\n",
    "        \n",
    "        Arguments:\n",
    "            x -- Tensor of shape (batch_size, input_seq_len, embedding_dim )\n",
    "            training -- Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            mask -- Boolean mask to ensure that the padding is not \n",
    "                    treated as part of the input\n",
    "        Returns:\n",
    "            decoder_layer_out -- Tensor of shape (batch_size, input_seq_len, embedding_dim)\n",
    "        \"\"\"\n",
    "        # START CODE HERE\n",
    "        # calculate self-attention using mha(~1 line). Dropout will be applied during training\n",
    "        attn_output = self.mha([x, x, x], mask=mask) # not sure how to apply mask here. \n",
    "        \n",
    "        # apply layer normalization on sum of the input and the attention output to get the  \n",
    "        # output of the multi-head attention layer (~1 line)\n",
    "        out1 = self.layernorm1(x+attn_output)  # (batch_size, input_seq_len, fully_connected_dim)\n",
    "\n",
    "        # pass the output of the multi-head attention layer through a ffn (~1 line)\n",
    "        ffn_output = self.ffn(out1)   # (batch_size, input_seq_len, fully_connected_dim)\n",
    "        \n",
    "        # apply dropout layer to ffn output during training (~1 line)\n",
    "        ffn_output =  self.dropout_ffn(ffn_output, training=training)\n",
    "        \n",
    "        # apply layer normalization on sum of the output from multi-head attention and ffn output to get the\n",
    "        # output of the encoder layer (~1 line)\n",
    "        decoder_layer_out = self.layernorm2(out1+ffn_output)  # (batch_size, input_seq_len, fully_connected_dim)\n",
    "        # END CODE HERE\n",
    "        \n",
    "        return decoder_layer_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The entire Encoder is starts by passing the target input to an embedding layer \n",
    "    and using positional encoding to then pass the output through a stack of\n",
    "    decoder Layers\n",
    "        \n",
    "    \"\"\" \n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, \n",
    "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=tf.keras.initializers.Constant(ffn_embeddings),\n",
    "                            trainable=False)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.embedding_dim)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps) \n",
    "                           for _ in range(self.num_layers)]\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        self.final_layer = tf.keras.Sequential([GlobalAveragePooling1D(), \n",
    "                                                Dense(fully_connected_dim, activation=\"relu\"), \n",
    "                                                Dense(1, activation='sigmoid')])\n",
    "        \n",
    "    \n",
    "    def call(self, x, training):\n",
    "        \"\"\"\n",
    "        Forward  pass for the Decoder\n",
    "        \n",
    "        Arguments:\n",
    "            x -- Tensor of shape (batch_size, target_seq_len, embedding_dim)\n",
    "            \n",
    "            training -- Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            \n",
    "        Returns:\n",
    "            x -- Tensor of shape (batch_size, 1), probability of positive sentiments \n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        mask = create_padding_mask(x)\n",
    "        attention_weights = {}\n",
    "        \n",
    "        # START CODE HERE\n",
    "        # create word embeddings \n",
    "        ec = np.sqrt(self.embedding_dim) \n",
    "        x = ec*self.embedding(x)\n",
    "        x += self.pos_encoding \n",
    "\n",
    "        # apply a dropout layer to x\n",
    "        x = self.dropout(x, training = training)\n",
    "\n",
    "        # use a for loop to pass x through a stack of decoder layers and update attention_weights (~4 lines total)\n",
    "        for i in range(self.num_layers):\n",
    "            # pass x and the encoder output through a stack of decoder layers and save the attention weights\n",
    "            # of block 1 and 2 (~1 line)\n",
    "            x = self.dec_layers[i](x, training, mask)\n",
    "\n",
    "         \n",
    "        x = self.final_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 400)]             0         \n",
      "_________________________________________________________________\n",
      "transformer (Transformer)    (None, 1)                 12635597  \n",
      "=================================================================\n",
      "Total params: 12,635,597\n",
      "Trainable params: 210,297\n",
      "Non-trainable params: 12,425,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NUM_HEADS = 5  # Number of attention heads\n",
    "NUM_LAYERS = 1 # 1 layer of transformer block \n",
    "FF_DIM = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = layers.Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "transformer_1 = Transformer(NUM_LAYERS, EMBEDDING_DIM, NUM_HEADS, FF_DIM, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "outputs = transformer_1(inputs)\n",
    "\n",
    "model_transformer_1 = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model_transformer_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1250/1250 [==============================] - 1034s 827ms/step - loss: 0.2879 - accuracy: 0.8781 - val_loss: 0.2964 - val_accuracy: 0.8791\n",
      "Epoch 2/2\n",
      "1250/1250 [==============================] - 1041s 832ms/step - loss: 0.2429 - accuracy: 0.9015 - val_loss: 0.2736 - val_accuracy: 0.8895\n"
     ]
    }
   ],
   "source": [
    "model_transformer_1.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"]) \n",
    "historytransformer_1 = model_transformer_1.fit(\n",
    "    X_train_padded, y_train, batch_size=32, epochs=2, validation_data=(X_test_padded, y_test)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 400)]             0         \n",
      "_________________________________________________________________\n",
      "transformer_1 (Transformer)  (None, 1)                 12682629  \n",
      "=================================================================\n",
      "Total params: 12,682,629\n",
      "Trainable params: 257,329\n",
      "Non-trainable params: 12,425,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NUM_HEADS = 3 # Number of attention heads\n",
    "NUM_LAYERS = 2 # 2 layers of decoder block \n",
    "FF_DIM = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = layers.Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "transformer_2 = Transformer(NUM_LAYERS, EMBEDDING_DIM, NUM_HEADS, FF_DIM, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "outputs2 = transformer_2(inputs)\n",
    "\n",
    "model_transformer_2 = keras.Model(inputs=inputs, outputs=outputs2)\n",
    "model_transformer_2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " 247/1250 [====>.........................] - ETA: 15:33 - loss: 0.3548 - accuracy: 0.8406"
     ]
    }
   ],
   "source": [
    "model_transformer_2.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"]) \n",
    "historytransformer_2 = model_transformer_2.fit(\n",
    "    X_train_padded, y_train, batch_size=32, epochs=2, validation_data=(X_test_padded, y_test)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
